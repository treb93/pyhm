import datetime
from datetime import timedelta
import logging
import math
from os import environ
import time

import click
import pandas as pd
import numpy as np
from skopt import gp_minimize
from skopt.space import Real, Integer, Categorical
from skopt.utils import use_named_args
from skopt.callbacks import CheckpointSaver
from skopt import load
import torch
from environment import Environment
from src.classes.dataloaders import DataLoaders
from src.classes.dataset import Dataset
from src.classes.graphs import Graphs
from src.get_dimension_dictionnary import get_dimension_dictionnary
from src.train_loop import train_loop
from parameters import Parameters
from src.max_margin_loss import max_margin_loss
from src.model.conv_model import ConvModel

from src.utils import save_txt, save_outputs, get_last_checkpoint
from src.utils_vizualization import plot_train_loss

from logging_config import get_logger

log = get_logger(__name__)


def train(dataset: Dataset,
        environment: Environment,
        parameters: Parameters,
        visualization: bool,
        check_embedding: bool,
        **search_params):
    """
    Function to find the best hyperparameter combination.

    Files needed to run
    -------------------
    All the files in the src.utils_data.DataPaths:
        It includes all the interactions between user, sport and items, as well as features for user, sport and items.
    If starting hyperparametrization from a checkpoint:
        The checkpoint file, generated by skopt during a previous hyperparametrization. The most recent file of
        the root folder will be fetched.

    Parameters
    ----------
    data :
        Object of class DataLoader, containing multiple arguments such as user_item_train dataframe, graph schema, etc.
    fixed_params :
        All parameters that are fixed, i.e. not part of the hyperparametrization.
    data_paths :
        All data paths (mainly csv).  # Note: currently, only paths.result_filepath is used here.
    visualization :
        Visualize results or not.  # Note: currently not used, visualization is always on or controlled by fixed_params.
    check_embedding :
        Visualize recommendations or not.  # Note: currently not used, controlled by fixed_params.
    **params :
        Mainly params that come from the hyperparametrization loop, controlled by skopt.

    Returns
    -------
    recall :
        Recall on the test set for the current combination of hyperparameters.

    Saves to files
    --------------
    logging of all experiments:
        All training logs are saved to result_filepath, including losses, metrics and examples of recommendations
        Plots of the evolution of losses and metrics are saved to the folder 'plots'
    best models:
        All models, fixed_params and params that yielded recall higher than 8% on specific item identifier or 20% on
        generic item identifier are saved to the folder 'models'
    """
    # Establish hyperparameters
    # Dimensions
    out_dim = {
        'Very Small': 32,
        'Small': 96,
        'Medium': 128,
        'Large': 192,
        'Very Large': 256}
    hidden_dim = {
        'Very Small': 64,
        'Small': 192,
        'Medium': 256,
        'Large': 384,
        'Very Large': 512}
    search_params['out_dim'] = out_dim[search_params['embed_dim']]
    search_params['hidden_dim'] = hidden_dim[search_params['embed_dim']]
    
    
    if environment.search_result_path != "":
        search_result_table = pd.read_pickle(environment.search_result_path)
    else:
        search_result_table = pd.DataFrame(columns = [*search_params.keys(), 'lost_patience', 'min_loss', 'precision'])
        environment.search_result_path = '../pickles/search_result_table.pkl'

    # Can't handle too big GNN for the moment.
    # TODO: Handle this cases
    if isSearchTooHeavy(search_params):
         return 0

    if search_params['aggregator_weighted'] == True:
        search_params['aggregator_type'] += '_weighted'

    parameters.update(search_params)
    
    graphs = Graphs(dataset, parameters)

    dim_dict = get_dimension_dictionnary(graphs, parameters)

    all_sids = None
    
    dataloaders = DataLoaders(graphs, dataset, parameters, environment)

    model = ConvModel(dim_dict, parameters)

    hp_sentence = search_params
    hp_sentence = f'{str(hp_sentence)[1: -1]} \n'

    save_txt(
        f'\n \n START - Hyperparameters \n{hp_sentence}',
        environment.result_filepath,
        "a")

    print(f'\n \n START - Hyperparameters \n{hp_sentence}')

    start_time = time.time()

    # Train model
    
    trained_model, viz, best_metrics = train_loop(
        model=model,
        graphs=graphs,
        dataset=dataset,
        dataloaders=dataloaders,
        loss_fn=max_margin_loss,
        get_metrics=True,
        parameters=parameters,
        environment=environment,
    )

    
    elapsed = time.time() - start_time
    result_to_save = f'\n {timedelta(seconds=elapsed)} \n END'
    save_txt(result_to_save, environment.result_filepath, mode='a')

    if visualization:
        plot_train_loss(hp_sentence, viz, parameters)

    # Report performance on validation set
    sentence = f"BEST VALIDATION Precision at 3 / 6 / 12 {best_metrics['precision_3'] * 100:.3f}% / {best_metrics['precision_6'] * 100:.3f}%  / {best_metrics['precision_12'] * 100:.3f}% "

    # log.info(sentence)
    save_txt(sentence, environment.result_filepath, mode='a')

    # Report performance on test set
    log.debug('Test metrics start ...')

    date = str(datetime.datetime.now())[:-10].replace(' ', '')
    save_outputs(
        {

            f'{date}_params': search_params,
            f'{date}_fixed_params': vars(parameters),
        },
        'models/'
    )
    
    # Save result into a dataframe.
    new_row = []
    for key in search_params.keys():
        new_row.append(search_params[key])
        
    new_row.append(True if 'lost_patience' in best_metrics.keys() else False)
    new_row.append(best_metrics['min_loss'])
    new_row.append((best_metrics['precision_6'] + best_metrics['precision_12']) / 2)
        
    new_row = pd.DataFrame([new_row], columns = [*search_params.keys(), 'lost_patience', 'min_loss', 'precision'])
    search_result_table = pd.concat([search_result_table, new_row], axis = 0) 
    
    search_result_table.to_pickle(environment.search_result_path)
    


    mean_precision = (best_metrics['precision_3'] + best_metrics['precision_6'] + best_metrics['precision_12']) / 3

    recap = f"BEST PRECISION on 1) Validation set : {mean_precision * 100:.2f}%"
    recap += f"\nLoop took {timedelta(seconds=elapsed)} for {len(viz['train_loss_list'])} epochs, an average of " \
             f"{timedelta(seconds=elapsed / len(viz['train_loss_list']))} per epoch"
    # print(recap)
    save_txt(recap, environment.result_filepath, mode='a')

    del dataloaders
    del graphs
    del model

    return mean_precision



class SearchableHyperparameters:
    """
    All hyperparameters to optimize.

    Attributes
    ----------
    Aggregator_hetero :
        How to aggregate messages from different types of edge relations. Choices : 'sum', 'max',
        'min', 'mean', 'stack'. More info here
        https://docs.dgl.ai/_modules/dgl/nn/pytorch/hetero.html
    Aggregator_type :
        How to aggregate neighborhood messages. Choices : 'mean', 'pool' for max pooling or 'lstm'
    Clicks_sample :
        Proportion of all clicks edges that should be used for training. Only relevant if
        fixed_params.train_on_clicks == True
    Days_popularity :
        Number of days considered in Use_popularity
    Dropout :
            Dropout used on nodes features (at all layers of the GNN)
    Embedding_layer :
        Create an explicit embedding layer that projects user & item features into and embedding
        of hidden_size dimension. If false, the embedding is done in the first layer of the GNN
        model.
    Purchases_sample :
        Proportion of all purchase (i.e. 'buys') edges that should be used for training. If
        fixed_params.discern_clicks == False, then 'clicks' edges are considered as 'purchases'
    Norm :
        Perform normalization after message aggregation
    Use_popularity :
        When computing ratings, add a score for items that were recent in the last X days
    Use_recency :
        When computing the loss, give more weights to more recent transactions
    Weight_popularity :
        Weight of the popularity score
    """

    def __init__(self):
        self.aggregator_hetero = Categorical(
            categories=[
                'mean', 
                'sum', 
                'max'
            ], name='aggregator_hetero')
        self.aggregator_type = Categorical(
            categories=[
                'mean',
                #'lstm',
                'mean_nn',
                'pool_nn'
                ],
            name='aggregator_type')  # LSTM?
        
        self.aggregator_weighted = Categorical(categories=[
            True, 
            False
        ], name='aggregator_weighted')
        self.delta = Real(low=0.15, high=0.35, prior='log-uniform',
                          name='delta')
        self.dropout = Real(low=0.05, high=0.3, prior='uniform',
                            name='dropout')
        self.embed_dim = Categorical(
            categories=[
                #'Very Small',
                #'Small',
                #'Medium',
                'Large',
                'Very Large'
                ],
            name='embed_dim')
        self.embedding_layer = Categorical(
            categories=[
                True, 
                False
            ], name='embedding_layer')
        self.lr = Real(low=1e-4, high=1e-3, prior='log-uniform', name='lr')
        self.n_layers = Categorical(
            categories=[
                2,
                3, 4
                #, 5
            ], name='n_layers')
        
        self.neg_sample_size = Integer(low=1500, high=2300,
                                       name='neg_sample_size')
        self.neighbor_sampling = Categorical(
            categories=[
                #1, 2, 3, 
                False
            ], name='neighbor_sampling')
        self.norm = Categorical(categories=[
            True, 
            # False
        ], name='norm')

        self.reduce_article_features = Categorical(
            categories=[
                True, 
                False
            ], name='reduce_article_features')

        
        # List all the attributes in a list.
        # This is equivalent to [self.hidden_dim_HP, self.out_dim_HP ...]
        self.dimensions = [self.__getattribute__(attr)
                           for attr in dir(self) if '__' not in attr]
        self.default_parameters = ['max', 'pool_nn', True, 0.3, 0.1,
                                   'Very Large', False, 0.00015, 2, 1600, False, True, False]


searchable_params = SearchableHyperparameters()
fitness_params = None

def isSearchTooHeavy(search_params: SearchableHyperparameters):
    """ Returns True if the Search parameters are going to blow up the computer. """
    n_layers = search_params['n_layers'] + (1 if search_params['embedding_layer'] == False else 0)
    

    if search_params['aggregator_type'] != 'mean':
        if(search_params['embed_dim'] == 'Very Large' and n_layers > 3):
            return True
        
        if(search_params['embed_dim'] == 'Large' and n_layers > 4):
            return True

    if(search_params['embed_dim'] == 'Medium' and n_layers > 4):
        return True
    
    if(search_params['aggregator_type'] == 'lstm'):
        return True

    

@use_named_args(dimensions=searchable_params.dimensions)
def fitness(**search_params):
    """
    Function used by skopt to find the best hyperparameter combination.

    The function calls the train function defined earlier, with all needed parameters. The precision that is returned
    is then multiplied by -1, since skopt is minimizing metrics.
    """
    precision = train(**{**fitness_params, **search_params})

    print(f"Get a precisin of {precision} with params: ", search_params)
    return -precision


@click.command()
@click.option('--from_beginning', count=True,
              help='Continue with last trained model or not')
@click.option('-v', '--verbose', count=True, help='Verbosity')
@click.option('-viz', '--visualization', count=True, help='Visualize result')
@click.option('--check_embedding', count=True, help='Explore embedding result')
@click.option('--remove', default=.95, help='Data remove percentage')
@click.option('--num_epochs', default=10, help='Number of epochs')
@click.option('--start_epoch', default=0, help='Start epoch')
@click.option('--patience', default=3, help='Patience for early stopping')
@click.option('--edge_batch_size', default=2048,
              help='Number of edges in a train / validation batch')
def main(from_beginning, verbose, visualization, check_embedding,
         remove, num_epochs, start_epoch, patience, edge_batch_size):
    """
    Main function that loads data and parameters, then runs hyperparameter loop with the fitness function.

    """
    global searchable_params
    
    if verbose:
        log.setLevel(logging.DEBUG)
    else:
        log.setLevel(logging.INFO)

    environment = Environment()

    parameters = Parameters({
        'num_epochs': num_epochs,
        'start_epoch': start_epoch,
        'patience': patience,
        'edge_batch_size': edge_batch_size,
        'remove': remove,
        'embedding_layer': True
    })

    checkpoint_saver = CheckpointSaver(
        f'checkpoint{str(datetime.datetime.now())[:-10]}.pkl',
        compress=9
    )


    print("Load dataset.")
    dataset = Dataset(environment, parameters)

    global fitness_params
    fitness_params = {
        'dataset': dataset,
        'environment': environment,
        'parameters': parameters,
        'visualization': visualization,
        'check_embedding': check_embedding,
    }
    
    if from_beginning:
        search_result = gp_minimize(
            func=fitness,
            dimensions=searchable_params.dimensions,
            n_calls=200,
            acq_func='EI',
            x0=searchable_params.default_parameters,
            callback=[checkpoint_saver],
            random_state=46,
            verbose = True
        )

    if not from_beginning:
        checkpoint_path = None
        if checkpoint_path is None:
            checkpoint_path = get_last_checkpoint()
        res = load(checkpoint_path)

        x0 = res.x_iters
        y0 = res.func_vals

        search_result = gp_minimize(
            func=fitness,
            dimensions=searchable_params.dimensions,
            n_calls=200,
            # Workaround suggested to correct the error when resuming training
            n_initial_points=-len(x0),
            acq_func='EI',
            x0=x0,
            y0=y0,
            callback=[checkpoint_saver],
            random_state=46,
            verbose = True
        )
        
    log.info(search_result)
    
    print("Search result : ", search_result)


if __name__ == '__main__':
    main()
